> Created on Tue May 26 21/07/04 2020  @author: Richie Bao-caDesign (cadesign.cn) __+updated on Mon Jul 13 10/10/44 2020 by Richie Bao

## 1. Standard error, central limit theorem, Student's t-distribution, statistical significance, effect size, confidence interval; Geospatial distribution and correlation analysis of public health data
### 1.1 Standard error, central limit theorem, Student's t-distribution, statistical significance, effect size, confidence interval
#### 1.1.1 Standard error

Definition: The standard error is the standard deviation of the sampling distribution (i.e., the standard deviation of the sample mean rather than the sample) of a statistical magnitude(such as the mean, the difference between two means, the correlation coefficient, etc.), which measures the expected random differences between samples of the same volume taken from the same population. In the following code, from the normal distribution with an average value of 30 and a standard deviation of 5, 2000 samples with each sample size of 1000 are randomly extracted, and the mean value of each sample is calculated to check the mean distribution fo these 2000 samples, namely the sampling distribution of the mean. A normal distribution has a mean and a standard deviation. In a sampling distribution of the mean, the mean is called the expected value of the mean; this is because the best guess for the sample mean is consistent with the population mean, which is proved by the following code, which calculates that the mean of the sampling distribution of the mean is almost the same as that of the population. The standard deviation of the sampling distribution of the mean is called the standard error. Standard deviation is the distribution of individual values and distribution of the average difference between the mean or average deviation; standard error of the mean provides the same information; only a single average difference between the sample mean and its expected value can be understood as the degree of confidence that the sample mean represents the actual population mean, which helps determine whether the difference between the sample statistics(such as the sample mean) and the population parameters(such as the population mean) is meaningful. Simultaneously, the 'kstest' normality test of the mean sampling distribution's standard score is calculated. The data generated by each code run is a random value under the above conditions. Its 'pvalue' is changeable, but usually higher than 0.5, that is, >0.05, so it can be judged that the sampling distribution of the mean conforms to the normal distribution.

Calculation formula of the standard error of mean value:

 $\sigma _{ \overline{x} }= \frac{ \sigma }{ \sqrt{n} } $或 $S _{ \overline{x} }= \frac{s }{ \sqrt{n} }$，where $\sigma$ is the population standard deviation, $s$ is the sample estimation of the standard deviation, and $n$ is the sample size. Since the standard deviation of $\sigma$ population is normally unknown, the standard error is calculated using a sample estimation of standard deviation.

> Reference：Timothy C.Urdan.Statistics in Plain English. Routledge; 3rd Edition (May 27, 2010)


```python
import numpy as np
from scipy import stats
import seaborn as sns
import math
sns.set()
mu,sigma=30,5
sample_size=2000
sample_mus=np.array([np.random.normal(mu, sigma, sample_size).mean() for i in range(2000)]) #From a normal distribution with a mean value of 30 and a standard deviation of 5, 2000 samples with a sample size of 1000 were randomly extracted. The mean value of each sample was calculated.
bins=30
sns.distplot(sample_mus,bins=bins) #Let us look at the distribution of our 2000 sample means.
print("sample_mus mu:%.2f,sigma:%.2f"%(sample_mus.mean(),sample_mus.std()))
kstest_test=stats.kstest(stats.zscore(sample_mus),cdf='norm')
print("sampling distribution of the mean - kstest statistic:%.2f,pvalue:%.2f"%kstest_test)
print("standard error of mean:",sample_mus.std()/math.sqrt(sample_size) ) #Calculate standard error
print("standard error of mean_scipy.stats.sem():",stats.sem(sample_mus,ddof=0)) #The standard error is calculated directly using the 'scipy.stats' method.
```

    sample_mus mu:30.00,sigma:0.11
    sampling distribution of the mean - kstest statistic:0.01,pvalue:0.96
    standard error of mean: 0.00248975473591512
    standard error of mean_scipy.stats.sem(): 0.00248975473591512
    


<a href=""><img src="./imgs/6_1.png" height="auto" width="auto" title="caDesign"></a>


The larger the size of a single sample is, the closer it is to the population, the closer the mean of the sampling distribution of the mean is to the population mean, and the smaller the standard deviation, that is, the smaller the standard error is, indicating that the statistic of the sample mean (or other statistics calculated) can represent the statistics of the population. The more accurate the estimation of the population is. The following code analyzes the above viewpoints and calculates the standard error of sampling distribution of multiple different sample sizes. With the increase of sample size, the standard error decreases rapidly and gradually tends to be alleviated, indicating that the larger a single sample size is, the closer it is to the population. The more accurate the estimation of the population is. 


```python
import matplotlib.pyplot as plt 
sample_mu_list=[]
sample_sigma_list=[]
sample_size_list=list(range(10,2000,20))
for sample_size in sample_size_list:
    sample_size_mu=np.array([np.random.normal(mu, sigma, sample_size).mean() for i in range(2000)])
    sample_mu_list.append(sample_size_mu.mean())
    sample_sigma_list.append(sample_size_mu.std())
ax=sns.lineplot(x=sample_size_list,y=sample_sigma_list)  
ax.set(xlabel='sample_size',ylabel='sample_sigma')
plt.show()
```


<a href=""><img src="./imgs/6_2.png" height="auto" width="auto" title="caDesign"></a>


#### 1.1.2 Central limit theorem
The central limit theorem shows that the sampling distribution of the mean is normal, even if the population distribution of the sample's values is not, provided the sample size is large enough. Or it can be understood that no matter what distribution the sample population obeys when the sample population is large enough, the sample average fluctuates around the population mean in the form of a normal distribution. To verify the central limit theorem, use Baidu POI data to extract the 'delicacy' field's price data. In the normal distribution and probability density function section, it has been confirmed that the delicacy price dataset is non-normal distribution through the normal test. The following codes reproduce the 'kstest' normal test, pvalue=5.153632014592587e-26, less than 0.05, reject the null hypothesis, that is, it does not conform to the normal distribution. A single sample size of 350 was randomly selected from the dataset, with 2000 samples. In the 'kstest' normal test after the sampling distribution of the mean, the pvalue is usually greater than 0.5; that is, the mean's sampling distribution is normal and meets the definition of the central limit theorem.


Many statistics depend on getting probabilities from the normal distribution, and the central limit theorem determines that enough sample (means) follow a normal distribution (even if the population distribution is not normal); that is, the sampling distribution of the mean is normal, so this makes the probability calculation of many statistics possible.


```python
import util
import pandas as pd
poi_gpd=pd.read_pickle('./data/poiAll_gpd.pkl') #Read the POI data already stored in .pkl format, including the 'geometry' field, as the GeoDataFrame geographic information data, which can be quickly viewed through 'poi_gpd.plot()'. 
delicacy_price=poi_gpd.xs('poi_0_delicacy',level=0).detail_info_price  #Extract delicacy price data
delicacy_price_df=delicacy_price.to_frame(name='price').astype(float) 
delicacy_price_df_clean=delicacy_price_df.dropna()
_,delicacyPrice_outliersDrop=util.is_outlier(delicacy_price_df_clean.price,threshold=3.5) #Place the outlier handler function in the 'util.py' file, and call it directly. Check the section on outlier handling.

delicacy_price_array_dropna=delicacyPrice_outliersDrop.to_numpy().reshape(-1)
print("original data mean:%.2f"%delicacy_price_array_dropna.mean())

print("original data kstest:",stats.kstest(stats.zscore(delicacy_price_array_dropna),cdf='norm')) #Calculate the standardized 'kstest' normality test

delicacy_price_sample_mus=np.array([np.random.choice(delicacy_price_array_dropna,350).mean() for i in range(2000)])
sns.distplot(delicacy_price_sample_mus,bins=bins)
print("sample_mus mu:%.2f,sigma:%.2f"%(delicacy_price_sample_mus.mean(),delicacy_price_sample_mus.std()))
kstest_test=stats.kstest(stats.zscore(delicacy_price_sample_mus),cdf='norm')
print("sample_mus kstest statistic:%.2f,pvalue:%.2f"%kstest_test)
```

    original data mean:47.70
    original data kstest: KstestResult(statistic=0.11464960111854455, pvalue=5.153632014592587e-26)
    sample_mus mu:47.70,sigma:1.67
    sample_mus kstest statistic:0.01,pvalue:0.93
    


<a href=""><img src="./imgs/6_3.png" height="auto" width="auto" title="caDesign"></a>


#### 1.1.3 Student's t-distribution
If you want to use a normal distribution and get an exact probability by z-score, you have to satisfy at least two conditions: $\sigma$ population standard deviation; The other is a large sample(i.e., a large enough sample size). If none of the above conditions are met, then the sample size needs to be considered, and then the Student's t-distribution is used. The shape of the t-distribution affected by sample size, large sample condition, the curve shape of the t-distribution and the normal distribution is the same, and with the sample size is reduced, the shape of the t-distribution among becoming more smooth, thicker on both ends, namely,  around the average values become less, and away from the mean, at the end of the distribution of values become more and more. To observe the t-distribution, take the example given by Scipy(scipy.stats.t) library. The code method is the same as the normal distribution, and the difference can be compared with each other.

In the Student's t-distribution, a new parameter, degree of freedom(df) is added, usually represented by the symbol $\nu $. When the sample statistics are used to estimate the population's parameters, the degree of freedom for N random samples is N-1.

The dataset is a normal distribution, given a value, the probability of the value can be obtained after calculating its z-score. Under Student's t-distribution, t value (t statistic) needs to be calculated to obtain the probability of corresponding value, and its formula is defined as: $t= \frac{ \overline{X} - \mu }{S _{ \overline{x} }} $,$\mu$ is the population mean,$\overline{X}$ is the sample mean，${S _{ \overline{x} }}$ is the sample estimation of the standard error.If you know the t value and the degree of freedom(sample size N minus 1), you can get the probability of this value by looking up the t value table.  Now, of course, the calculations are done directly using libraries such as Scipy, without having to compute t value and corresponding lookups.


```python
from scipy.stats import norm
from scipy.stats import t
import matplotlib.pyplot as plt
fig, ax=plt.subplots(1, 1)
df=2.74 #Degree of freedom in configuration
mean, var, skew, kurt=t.stats(df, moments='mvsk') #Look at the statistics that follow the t-distribution
print('mean, var, skew, kurt=',(mean, var, skew, kurt))

x=np.linspace(t.ppf(0.01, df),t.ppf(0.99, df), 100) #Obtain 100 values with probabilities ranging from 1% to 99%, subject to DOF
ax.plot(x, t.pdf(x, df),'r-', lw=5, alpha=0.6, label='t_2.75 pdf')

rv=t(df) #Specified fixed degree of freedom
ax.plot(x, rv.pdf(x), 'k-', lw=2, label='t_2.75 frozen pdf')

rv_10=t(0.2) #Specified fixed degree of freedom
ax.plot(x, rv_10.pdf(x), 'g-', lw=2, label='t_0.2 pdf')

vals=t.ppf([0.001, 0.5, 0.999], df) #Returns values with probabilities of 0.1%, 50% and 99.9%
print("Verify that the cumulative distribution function(CDF) return value is equal to or approximate to the PPF return value:",np.allclose([0.001, 0.5, 0.999], t.cdf(vals, df)))

r=t.rvs(df, size=1000) #Obtain 1000 random values of df subject to the degree of freedom
ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)


#Compare the standard normal distribution
x_nd=np.linspace(norm.ppf(0.01),norm.ppf(0.99), 100)
ax.plot(x, norm.pdf(x), 'b--', lw=2, alpha=0.8, label='norm pdf')

ax.legend(loc='best', frameon=False)
plt.show()
```

    mean, var, skew, kurt= (array(0.), array(3.7027027), array(nan), array(inf))
    Verify that the cumulative distribution function(CDF) return value is equal to or approximate to the PPF return value: True
    


<a href=""><img src="./imgs/6_4.png" height="auto" width="500" title="caDesign"></a>



```python
#If you want to calculate the probability of a Student's t-distribution, specify df, loc(mean, default is 0), and scale(standard deviation, default is 1). 
print("Using .cdf to calculate the probability that the value is less than or equal to 113 is:",t.cdf(113,df=999,loc=100,scale=12)) 
print("Using .sf to calculate the probability that the value is greater than or equal to 113 is:",t.sf(113,df=999,loc=100,scale=12)) 
print(" It can be observed the probability when .cdf(<=113) +.sf((>=113) :",t.cdf(113,df=999,loc=100,scale=12)+t.sf(113,df=999,loc=100,scale=12))
print("Using .ppf to find the value with a given probability value of 98% is:",t.ppf(0.81766,df=999,loc=100,scale=12))
```

    Using .cdf to calculate the probability that the value is less than or equal to 113 is: 0.8605390547558804
    Using .sf to calculate the probability that the value is greater than or equal to 113 is: 0.13946094524411956
     It can be observed the probability when .cdf(<=113) +.sf((>=113) : 1.0
    Using .ppf to find the value with a given probability value of 98% is: 110.88276310459135
    

#### 1.1.4 statistical significance
The Scipy library was used to establish a normal distribution with a mean of 100 and a standard deviation of 12. Draw the random value distribution of a single sample with a sample size of 1000. Simultaneously, the mean sampling distribution of 2000 samples with a sample size of 1000 is plotted. The standard deviation of the sampling distribution of the 2000 sample means is calculated as the standard deviation of the t-distribution, the mean is not changed to 100, and the degree of freedom is 1999(i.e., the sample size is 2000, which is equal to the number of samples of the sampling distribution of the mean). By observing the mean sampling distribution and the corresponding t-distribution (the same as the mean and standard deviation, and the number of samples with degrees of freedom as the mean sampling distribution), the curve shape is consistent, and the central limit theorem is verified once again. In such 2000 samples with a capacity of 1000, the probability of the sample mean can be estimated according to the t-distribution, which is  99.387 if less than 0.05(5%) and 100.641 if greater than 0.95(95%). Therefore, we can estimate the probability of a sample mean less than 99.387 or greater than 100.641 to be less than 5%(0.05).

Suppose instead of getting the corresponding value according to the probability(that is, the sample mean), we estimate the value's probability, for example. In that case, the probability that the sample mean is less than or equal to 99.0, and the result is 0.00439. Does this probability value determine whether the difference between the sample statistics(e.g., the sample mean) and the population parameters(e.g., the population mean)(e.g., the difference between the sample mean 99.0 and the population mean 100 is 00-99.0=1.0) is simply due to random sampling error or chance? The convention is to take the probability p-value=0.05 as the boundary of a level, which is, namely, the probability of the occurrence of sample observation results or more extreme results obtained when the null hypothesis is true, and its p-value is small, indicating that the probability of the occurrence of the null hypothesis is very small. According to the principle of small probability, 0.00439<0.05, there is a reason to reject the null hypothesis, the smaller the p-value, reject the null hypothesis of the reasons more fully, that is to say, the difference between the sample mean 99.0 and the population mean is not accidental. It is concluded that the sample average of 99.0 is different from the population mean, and the correlation observed in this sample does not represent the actual phenomenon in the population.

For the above process, which can be transformed into the inference process of hypothesis testing, a hypothesis is proposed, as well as a criterion is established to decide whether to retain or reject the hypothesis. Assuming that the sample mean is not different from the population mean, it is a null hypothesis($H_{0} $). The null hypothesis generally means that the population's effect does not exist(the population average and the sample mean will not be different); the symbol is $H_{0} : \mu = \overline{X} $, $ \mu$ is the population mean, $\overline{X}$ is the sample mean. If it is assumed that the sample mean is different from the population mean, it is an opposing hypothesis(an alternative hypothesis), denoted by: $H_{A} : \mu \neq \overline{X} $. The sample mean tends to be the same as the population mean, but because it is not the population, after all, there is always a difference between the sample and the population. The reason for that difference is the random sampling error or the chance factor, which is the inherent reason for the mean sampling distribution normality. p-value<=0.05 probability that is corresponding to the significance level(α，alpha level), such as the probability of the sample mean of 99.0 is 0.00439, less than the significance level α, reject the null hypothesis, the sample mean is different from the population mean, the result is statistically significant(refers to under the condition of the original hypothesis is true, the sample statistics values used to test fall within the rejection region, has decided to reject the null hypothesis). In addition to 0.05, there was also a 0.01 significance level.

There are two cases in the above calculation process with a probability of less than 0.025 and greater than 0.975. If both cases are included at the same time, it is double-tailed detection. There is only one case, but the probability of less than 0.05 or greater than 0.95 is single-tailed detection.


```python
import matplotlib.pyplot as plt
df,loc,scale,sample_size=1999,100,12,1000

#Plot the random value distribution of a single sample with a sample size of 1000
x=np.linspace(norm.ppf(0.001, loc=loc,scale=scale),norm.ppf(0.999, loc=loc,scale=scale), sample_size)
fig, axs=plt.subplots(1,3,figsize=(27,5))
axs[0].plot(x, norm.pdf(x,loc=loc,scale=scale),'r-', lw=2, alpha=0.8, label='nd-the distribution \n of individual sample values')
axs[0].legend(loc='upper left', frameon=False)

#Plot the sampling distribution of the mean of 2000 samples with a sample size of 1000
samples=np.array([norm.rvs(loc=loc,scale=scale,size=sample_size).mean() for i in range(df+1)])
bins=30
sns.distplot(samples,bins=bins,ax=axs[1],label='sampling distribution of the mean')
axs[1].legend(loc='upper left', frameon=False)

#The standard deviation of the sampling distribution of the mean of 2000 is calculated as the standard deviation of the t-distribution, the mean remains unchanged at 100, and the degree of freedom is 1999(that is, the sample size is 2000, which is equal to the number of samples of the sampling distribution of the mean).
samples_std=samples.std()
t_x=np.linspace(t.ppf(0.001, df=df,loc=loc,scale=samples_std),t.ppf(0.999, df=df,loc=loc,scale=samples_std), df+1)  #Obtain 100 values subject to the degree of freedom with probabilities ranging from 1% to 99%
t_rv=t(df=df,loc=loc,scale=samples_std) #Specified fixed degree of freedom
axs[2].plot(t_x, t_rv.pdf(t_x), 'g-', lw=2, label='t-$mu$=%d,sigma=%d'%(loc,scale))

#When the significance level is 0.05, the curve interval is drawn.
pValue_5percent=t.ppf(0.025,df=df,loc=loc,scale=samples_std)
pValue_95percent=t.ppf(0.975,df=df,loc=loc,scale=samples_std)
axs[2].axvline(pValue_5percent, 0,0.3,c='gray',label='alpha level')
axs[2].axvline(pValue_95percent, 0,0.3,c='gray')
axs[2].annotate('reject null hypothesis,a=%.3f'%0.025,xy=(pValue_5percent,0.3),xytext=(pValue_5percent-0.1, 0.5),arrowprops=dict(facecolor='black', shrink=0.01),horizontalalignment='right',)
axs[2].annotate('reject null hypothesis,a=%.3f'%0.025,xy=(pValue_95percent,0.3),xytext=(pValue_95percent+0.07, 0.55),arrowprops=dict(facecolor='black', shrink=0.01),horizontalalignment='left',)
axs[2].legend(loc='upper left', frameon=False)

plt.show()

print("Using .ppf to find the value with a given probability value of 5% is :",pValue_5percent)
print("Using .ppf to find the value with a given probability value of 95% is :",pValue_95percent)
print("Using .cdf to calculate the probability that the value is less than or equal to 99 is:",t.cdf(99.0,df=df,loc=loc,scale=samples_std))
```


<a href=""><img src="./imgs/6_5.png" height="auto" width="auto" title="caDesign"></a>


    Using .ppf to find the value with a given probability value of 5% is : 99.27672192134695
    Using .ppf to find the value with a given probability value of 95% is : 100.72327807865305
    Using .cdf to calculate the probability that the value is less than or equal to 99 is: 0.003377883090124475
    

#### 1.1.5 Effect size
Effect size is a value that quantifies the strength of a phenomenon. The larger the absolute value, the stronger the effect, the more obvious the phenomenon. For the sampling distribution of the mean, the calculation formula of the effect size is :$d=\frac{ \overline{X} - \mu }{s}$，$d$ is the effect size, $ \overline{X}$ is the sample mean, $\mu$ is the population mean, and $s$ is the sample estimation of the standard deviation. The effect size represents the difference measured in standard deviation, which is very similar to the standard score. Its unit is the standard deviation, how many standard deviations are there from the point of interest to the mean. When it comes to whether the effect size is meaningful, the recommended effect size interval also varies, and the key is to test what and what views are held. Generally speaking, the effect size is small if it is less than 0.20, medium if it is between 0.25 and 0.75, and large if it is more than 0.80. The calculation result of the effect size of the sample mean 99 is -2.613, and its absolute value is greater than 0.8, indicating that the difference between the sample mean 99 and the population mean 100 is 100-99=1. The effect size is significant in terms of standard deviation; that is, the sample mean 99 is far away from the population mean 100.


```python
print("The effect size of the sample mean 99:",(99-100)/samples_std)
```

    The effect size of the sample mean 99: -2.7114763713325534
    

#### 1.1.6 Confidence interval
The Confidence interval(CI) of a probabilistic sample is estimated in the form of an interval for a position parameter value of the population parametric distribution for producing the sample. For a real sampling distribution fo the mean, we do not know the population mean(the actual value of the population parameter). Still, all we have is sample data. We can estimate the population's mean from the sample data and give the interval of distribution of the population mean, and this interval is the confidence interval. The calculation formula is:$C I_{95}= \overline{X}   \pm ( t_{95} )( s_{ \overline{X} })$ 和$C I_{99}= \overline{X}   \pm ( t_{99} )(s_{ \overline{X} } )$，$C I_{95}$ is 95% confidence interval, $C I_{99}$ is 99% confidence interval, $\overline{X}$ is the sample mean, $ s_{ \overline{X} }$ is the standard error,  $t_{95} $ is the t-value corresponding to the two-tail test with the $a$ level of 0.05 under the given degree of  freedom condition,  $t_{99} $ is the t-value corresponding to the two-tail test with the $a$ level of 0.01 under the given degree of freedom condition. 

The Scipy library can be used directly for confidence interval calculations.


```python
print("0.05_Confidence interval: ",stats.t.interval(0.95, len(samples)-1, loc=np.mean(samples), scale=stats.sem(samples)))
print("0.01_Confidence interval: ",stats.t.interval(0.99, len(samples)-1, loc=np.mean(samples), scale=stats.sem(samples)))
```

    0.05_Confidence interval:  (99.98168869487951, 100.01404276341803)
    0.01_Confidence interval:  (99.97659806886004, 100.0191333894375)
    

### 1.2 Correlation
#### 1.2.1 Correlation index and correlation coefficient

If you want to determine whether two variables are related to each other, you need to calculate the correlation coefficient. Three situations involved the correlation analysis between variables, including numerical data and numerical data, numerical data and classified data, and classified data and classified data. Therefore, the correlation calculation index between different variable types is different:

| datatype | index  | value range | calculation formula  | description  |
|---|---|---|---|---|
| numerical data and numerical data  | correlation coefficient  |  -1~1 |$ r=\frac{ S_{xy} }{ \sqrt{ S_{xx}\cdot S_{yy} } } = \frac{ \sum_{i=1}^n ( x_{i}-  \overline{x}   )( y_{i}- \overline{y}  ) }{  \sqrt{ \sum_{i=1}^n ( x_{i} -\overline{x})^{2} } \cdot \sum_{i=1}^n ( y_{i} -\overline{y})^{2} }  \in [-1,1] $ |  $S_{xx}$ is called the variance of $x$, $S_{yy}$ is called the variance of $y$, $S_{xy}$ is called the covariance of $x$ and $y$|
| numerical data and classified data | relevance ratio  | 0~1  |  $WSS= \sum_{k=1}^K S_{ x^{k}  x^{i} } = \sum_{k=1}^K  \sum_{i=1}^{ n_{i} }  ( x_i^k - \overline{ x^{k} }  ) ^{2}  $ $BSS= \sum_{k=1}^K n_{k}  ( \overline{ x^{k} } - \overline{x} )^{2}  $ $cr=\frac{BSS}{BSS+WSS} $ | There are n numerical data, divided into K categories,$x_1^1,x_2^1, \cdots, x_{ n_{1} } ^1;x_1^2,x_2^2, \cdots, x_{ n_{2} } ^2;\cdots;x_1^K,x_2^K, \cdots, x_{ n_{K} } ^K$,$n_{K} $ represents the number of the value of the $K$ category, denoting $ \overline{ x^{k}}$ is the average value of the K category, $\overline{x}$ is the average value of all data. BSS (between sum of squares), WSS(within sum of squares) |
| classified data and classified data | Cramér's V  | 0~1  | $ \chi ^{2} = \sum_{i,j}^{}  \frac{ ( n_{ij}- \frac{ n_{i}  n_{j} }{n}  )^{2} }{ \frac{ n_{i}  n_{j} }{n} } $ $V= \sqrt{ \frac{  \chi ^{2}/n }{min(k-1,r-1)} } $ |Set two category variables A and B, the total number of observed samples is $n$，for $i=1,\cdots, r;j=1,\cdots,k $， $n_{ij}$ 为$( A_{i}, B_{j}  )$observation times(frequency), $k$ is the column of the observation order, and $r$ is its row. $ \frac{ n_{i} n_{j} }{n}$ is the expected count.|

For the correlation analysis between numerical data and numerical data, the correlation coefficient is calculated(usually Pearson's r is a linear correlation). A simple dataset was created using data from *The Manga Guide to Statistics*, which asked ten women in their 20s how much they spent on cosmetics and clothes. Taking the two variables to be analyzed as x and y values and plotting scatter to observe whether there is a law in the distribution of points, it can be observed clearly that the points' distribution seems to be along an invisible inclined straight line. It can be preliminarily judged that there is a correlation between cosmetics and clothes.

> Pearson correlation coefficient, also known as product difference correlation coefficient, Pearson product-moment correlation coefficient(PPMCC or PCCs)


```python
import pandas as pd
dressUp_cost={'name':["miss_A","miss_B","miss_C","miss_D","miss_E","miss_F","miss_G","miss_H","miss_I","miss_J"],
              "cosmetics_fee":[3000,5000,12000,2000,7000,15000,5000,6000,8000,10000],
             "clothes_fee":[7000,8000,25000,5000,12000,30000,10000,15000,20000,18000]
             }
dressUp_cost_df=pd.DataFrame.from_dict(dressUp_cost)
dressUp_cost_df.plot.scatter(x='cosmetics_fee',y='clothes_fee',c='DarkBlue')
plt.show()
```

<a href=""><img src="./imgs/6_6.png" height="auto" width="auto" title="caDesign"></a>


Use the above formula to calculate the correlation coefficient.

> The process of manual calculation helps to understand concepts and formulas.


```python
import math
dressUp_cost_df["xSx_mean"]=dressUp_cost_df.cosmetics_fee.apply(lambda row: row-dressUp_cost_df.cosmetics_fee.mean()) 
dressUp_cost_df["ySy_mean"]=dressUp_cost_df.clothes_fee.apply(lambda row: row-dressUp_cost_df.clothes_fee.mean())
dressUp_cost_df["square_xSx_mean"]=dressUp_cost_df.xSx_mean.apply(lambda row: math.pow(row,2))
dressUp_cost_df["square_ySy_mean"]=dressUp_cost_df.ySy_mean.apply(lambda row: math.pow(row,2))
dressUp_cost_df["xSx_meanMySy_mean"]=dressUp_cost_df.xSx_mean*dressUp_cost_df.ySy_mean
print(dressUp_cost_df)
r=dressUp_cost_df.xSx_meanMySy_mean.sum()/math.sqrt(dressUp_cost_df.square_xSx_mean.sum()*dressUp_cost_df.square_ySy_mean.sum())
print("Pearson's r:",r)
```

         name  cosmetics_fee  clothes_fee  xSx_mean  ySy_mean  square_xSx_mean  \
    0  miss_A           3000         7000   -4300.0   -8000.0       18490000.0   
    1  miss_B           5000         8000   -2300.0   -7000.0        5290000.0   
    2  miss_C          12000        25000    4700.0   10000.0       22090000.0   
    3  miss_D           2000         5000   -5300.0  -10000.0       28090000.0   
    4  miss_E           7000        12000    -300.0   -3000.0          90000.0   
    5  miss_F          15000        30000    7700.0   15000.0       59290000.0   
    6  miss_G           5000        10000   -2300.0   -5000.0        5290000.0   
    7  miss_H           6000        15000   -1300.0       0.0        1690000.0   
    8  miss_I           8000        20000     700.0    5000.0         490000.0   
    9  miss_J          10000        18000    2700.0    3000.0        7290000.0   
    
       square_ySy_mean  xSx_meanMySy_mean  
    0       64000000.0         34400000.0  
    1       49000000.0         16100000.0  
    2      100000000.0         47000000.0  
    3      100000000.0         53000000.0  
    4        9000000.0           900000.0  
    5      225000000.0        115500000.0  
    6       25000000.0         11500000.0  
    7              0.0               -0.0  
    8       25000000.0          3500000.0  
    9        9000000.0          8100000.0  
    Pearson's r: 0.968019612860768
    

Calculated directly using the Scipy library, the results are consistent.


```python
from scipy import stats
r_=stats.pearsonr(dressUp_cost_df.cosmetics_fee,dressUp_cost_df.clothes_fee)
print(
    "pearson's r:",r_[0],"\n",
    "p_value:",r_[1]
     )
```

    pearson's r: 0.968019612860768 
     p_value: 4.402991448166131e-06
    

There is no strict regulation on the strength of the correlation coefficient, which usually needs to be determined according to the specific application background and purpose, such as the variable affected by complex and changeable factors. The value of 0.9 is quite high. As for the significance of the correlation coefficient, different literature's reference significance is also different. Here is the division in *The Manga Guide to Statistics*:

| The absolute value of the correlation coefficient  | subdivided  | roughly divided  |  
|---|---|---|
| 1.0~0.9  |  very strong | correlated |   
| 0.9~0.7  | a little bit stronger  |  correlated |   
|0.7~0.5   | a little weak |  correlated |   
| less than 0.5  |  very weak | uncorrelated |   


As well as a reference from Wikipedia:

| correlation  | minus  | plus  |  
|---|---|---|
| none  | −0.09 to 0.0 | 0.0 to 0.09  |   
| weak  | −0.3 to −0.1  | 0.1 to 0.3 |   
|median   |−0.5 to −0.3  | 0.3 to 0.5 |   
| strong  | −1.0 to −0.5 |0.5 to 1.0   | 

If the correlation is close to $ \pm1 $, the stronger the correlation is, if it is close to 0, the weaker the correlation is. If it is positive, it is a positive correlation; it is a negative correlation if it is negative.

By calculating the correlation coefficient, the result is r=0.968, indicating a strong correlation between the two variables of cosmetics cost and clothes cost in the ten women in their 20s sampled. Does this value represent that there is also a strong correlation between the two variables(cosmetics cost and clothes cost) in the sample population(all women)? To confirm this phenomenon, we need to test whether the correlation coefficient is statistically significant. The proposed null hypothesis represents that the two variables(cosmetics cost and clothes cost) are completely uncorrelated in the whole population; that is, the whole population's correlation coefficient is 0. t-distribution is usually used to test whether the correlation coefficient is statistically significant, and t-test is performed. The calculation formula of the t-value is:$t=r \sqrt{ \frac{N-2}{1- r^{2} } } $，the degree of freedom is $N-2$, that is, the number of sample objects minus 2, that is, N-2=10-2=8. Calculate the p-value using the cumulative distribution function of the t-distribution t.cdf() or t.sf(), and multiply by 2 because of the double-tailed check, the result is consistent with the p-value given when using stats.pearsonr() to calculate the correlation coefficient. Because p_value=4.402991448104743e-06<0.05, therefore, reject the null hypothesis, that is, reject the two variables in the population (cosmetics cost and clothes cost) are completely uncorrelated, that is to say, the two variables in the population are correlated. Therefore, the calculated correlation value of 0.968 indicates a strong correlation between cosmetics expense and clothes expense for all women(in the population). If a woman spends more money on cosmetics, she also spends more money on clothes and vice versa.


```python
t_value=r*math.sqrt((10-2)/(1-math.pow(r,2)))
print("p_value_cdf:",(1-stats.t.cdf(t_value,10-2))*2)
print("p_value_sf:",stats.t.sf(t_value,10-2)*2)
```

    p_value_cdf: 4.402991448104743e-06
    p_value_sf: 4.402991448166121e-06
    

### 1.3 Geospatial distribution of public health data and correlation analysis
#### 1.3.1 Geospatial distribution of public health data 
 Public health data are the public health indicators selected by the Chicago community. The dataset includes 27 important public health indicators, such as ratios, percentages, birth and death rates, infectious diseases, lead poisoning, and economic conditions indicators, counted by the community with the community name given in the field. The data can be matched to the community-wide geospatial data by community name.  Chicago community boundaries and public health data are derived from  [Chicago Data Portal，CDP](https://data.cityofchicago.org/). A detailed description of the data can be found in the documentation provided by CDP.

There are two ways of printing data in JupyterLab, including 'print()' and placing variables of the data displayed directly on a single cell. But the former display for fixed-width can not automatically zoom following the width of the screen. However, the latter is 100% automatically zoom display, but if you want to export .ipynb Juypter file to .md Markdown file, it may produce confused characters. Therefore, you can use `from IPython.display import HTML`, which solves both of these problems when the data displayed is converted to HTML format.


```python
import pandas as pd
dataFp_dic={
    "ublic_Health_Statistics_byCommunityArea_fp":r'./data/Public_Health_Statistics-_Selected_public_health_indicators_by_Chicago_community_area.csv',
    "Boundaries_Community_Areas_current":r'./data/geoData/Boundaries_Community_Areas_current.shp',    
}

pubicHealth_Statistic=pd.read_csv(dataFp_dic["ublic_Health_Statistics_byCommunityArea_fp"])

#Chinese-English comparison table(Field mapping table)
PubicHealth_Statistic_columns={'Community Area':'社区', 
                                'Community Area Name':'社区名',
                                'Birth Rate':'出生率',
                                'General Fertility Rate':'一般生育率',
                                'Low Birth Weight':'低出生体重',
                                'Prenatal Care Beginning in First Trimester':'产前3个月护理', 
                                'Preterm Births':'早产',
                                'Teen Birth Rate':'青少年生育率',
                                'Assault (Homicide)':'攻击（杀人）',
                                'Breast cancer in females':'女性乳腺癌',
                                'Cancer (All Sites)':'癌症', 
                                'Colorectal Cancer':'结肠直肠癌',
                                'Diabetes-related':'糖尿病相关',
                                'Firearm-related':'枪支相关',
                                'Infant Mortality Rate':'婴儿死亡率', 
                                'Lung Cancer':'肺癌',
                                'Prostate Cancer in Males':'男性前列腺癌',
                                'Stroke (Cerebrovascular Disease)':'中风(脑血管疾病)',
                                'Childhood Blood Lead Level Screening':'儿童血铅水平检查',
                                'Childhood Lead Poisoning':'儿童铅中毒',
                                'Gonorrhea in Females':'女性淋病', 
                                'Gonorrhea in Males':'男性淋病', 
                                'Tuberculosis':'肺结核',
                                'Below Poverty Level':'贫困水平以下', 
                                'Crowded Housing':'拥挤的住房', 
                                'Dependency':'依赖',
                                'No High School Diploma':'没有高中文凭', 
                                'Per Capita Income':'人均收入',
                                'Unemployment':'失业',
                                }

def print_html(df,row_numbers=5):
    from IPython.display import HTML
    '''
    function - Print DataFrame format data in Jupyter as HTML
    
    Paras:
    df - Data in either DataFrame or GeoDataFrame format that needs to be printed
    row_numbers - The number of lines to print, if positive, from the start; If negative, print from the end
     '''
    if row_numbers>0:
        return HTML(df.head(row_numbers).to_html())
    else:
        return HTML(df.tail(abs(row_numbers)).to_html())
print_html(pubicHealth_Statistic,6)
```




<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Community Area</th>
      <th>Community Area Name</th>
      <th>Birth Rate</th>
      <th>General Fertility Rate</th>
      <th>Low Birth Weight</th>
      <th>Prenatal Care Beginning in First Trimester</th>
      <th>Preterm Births</th>
      <th>Teen Birth Rate</th>
      <th>Assault (Homicide)</th>
      <th>Breast cancer in females</th>
      <th>Cancer (All Sites)</th>
      <th>Colorectal Cancer</th>
      <th>Diabetes-related</th>
      <th>Firearm-related</th>
      <th>Infant Mortality Rate</th>
      <th>Lung Cancer</th>
      <th>Prostate Cancer in Males</th>
      <th>Stroke (Cerebrovascular Disease)</th>
      <th>Childhood Blood Lead Level Screening</th>
      <th>Childhood Lead Poisoning</th>
      <th>Gonorrhea in Females</th>
      <th>Gonorrhea in Males</th>
      <th>Tuberculosis</th>
      <th>Below Poverty Level</th>
      <th>Crowded Housing</th>
      <th>Dependency</th>
      <th>No High School Diploma</th>
      <th>Per Capita Income</th>
      <th>Unemployment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>Rogers Park</td>
      <td>16.4</td>
      <td>62.0</td>
      <td>11.0</td>
      <td>73.0</td>
      <td>11.2</td>
      <td>40.8</td>
      <td>7.7</td>
      <td>23.3</td>
      <td>176.9</td>
      <td>25.3</td>
      <td>77.1</td>
      <td>5.2</td>
      <td>6.4</td>
      <td>36.7</td>
      <td>21.7</td>
      <td>33.7</td>
      <td>364.7</td>
      <td>0.5</td>
      <td>322.5</td>
      <td>423.3</td>
      <td>11.4</td>
      <td>22.7</td>
      <td>7.9</td>
      <td>28.8</td>
      <td>18.1</td>
      <td>23714</td>
      <td>7.5</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>West Ridge</td>
      <td>17.3</td>
      <td>83.3</td>
      <td>8.1</td>
      <td>71.1</td>
      <td>8.3</td>
      <td>29.9</td>
      <td>5.8</td>
      <td>20.2</td>
      <td>155.9</td>
      <td>17.3</td>
      <td>60.5</td>
      <td>3.7</td>
      <td>5.1</td>
      <td>36.0</td>
      <td>14.2</td>
      <td>34.7</td>
      <td>331.4</td>
      <td>1.0</td>
      <td>141.0</td>
      <td>205.7</td>
      <td>8.9</td>
      <td>15.1</td>
      <td>7.0</td>
      <td>38.3</td>
      <td>19.6</td>
      <td>21375</td>
      <td>7.9</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>Uptown</td>
      <td>13.1</td>
      <td>50.5</td>
      <td>8.3</td>
      <td>77.7</td>
      <td>10.3</td>
      <td>35.1</td>
      <td>5.4</td>
      <td>21.3</td>
      <td>183.3</td>
      <td>20.5</td>
      <td>80.0</td>
      <td>4.6</td>
      <td>6.5</td>
      <td>50.5</td>
      <td>25.2</td>
      <td>41.7</td>
      <td>353.7</td>
      <td>0.5</td>
      <td>170.8</td>
      <td>468.7</td>
      <td>13.6</td>
      <td>22.7</td>
      <td>4.6</td>
      <td>22.2</td>
      <td>13.6</td>
      <td>32355</td>
      <td>7.7</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>Lincoln Square</td>
      <td>17.1</td>
      <td>61.0</td>
      <td>8.1</td>
      <td>80.5</td>
      <td>9.7</td>
      <td>38.4</td>
      <td>5.0</td>
      <td>21.7</td>
      <td>153.2</td>
      <td>8.6</td>
      <td>55.4</td>
      <td>6.1</td>
      <td>3.8</td>
      <td>43.1</td>
      <td>27.6</td>
      <td>36.9</td>
      <td>273.3</td>
      <td>0.4</td>
      <td>98.8</td>
      <td>195.5</td>
      <td>8.5</td>
      <td>9.5</td>
      <td>3.1</td>
      <td>25.6</td>
      <td>12.5</td>
      <td>35503</td>
      <td>6.8</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>North Center</td>
      <td>22.4</td>
      <td>76.2</td>
      <td>9.1</td>
      <td>80.4</td>
      <td>9.8</td>
      <td>8.4</td>
      <td>1.0</td>
      <td>16.6</td>
      <td>152.1</td>
      <td>26.1</td>
      <td>49.8</td>
      <td>1.0</td>
      <td>2.7</td>
      <td>42.4</td>
      <td>15.1</td>
      <td>41.6</td>
      <td>178.1</td>
      <td>0.9</td>
      <td>85.4</td>
      <td>188.6</td>
      <td>1.9</td>
      <td>7.1</td>
      <td>0.2</td>
      <td>25.5</td>
      <td>5.4</td>
      <td>51615</td>
      <td>4.5</td>
    </tr>
    <tr>
      <th>5</th>
      <td>6</td>
      <td>Lake View</td>
      <td>13.5</td>
      <td>38.7</td>
      <td>6.3</td>
      <td>79.1</td>
      <td>8.1</td>
      <td>15.8</td>
      <td>1.4</td>
      <td>20.1</td>
      <td>126.9</td>
      <td>13.0</td>
      <td>38.5</td>
      <td>1.8</td>
      <td>2.2</td>
      <td>32.5</td>
      <td>17.0</td>
      <td>24.4</td>
      <td>179.2</td>
      <td>0.4</td>
      <td>81.8</td>
      <td>357.6</td>
      <td>3.2</td>
      <td>10.5</td>
      <td>1.2</td>
      <td>16.5</td>
      <td>2.9</td>
      <td>58227</td>
      <td>4.7</td>
    </tr>
  </tbody>
</table>


When data is merged, public health data is sorted by "Community Area," and community boundary data is sorted by "area_numbe".  the 'df.merge()' method is used to complete the fusion. Two sets of data were printed, including Lung Cancer per 100,000 people. When the GeoDataFrame format data is printed, if you need to annotate each polygon on the graph and increase the amount of information on the map, you can process it as the corresponding line in the following code.


```python
import geopandas as gpd
community_area=gpd.read_file(dataFp_dic["Boundaries_Community_Areas_current"])
print(community_area.dtypes) 
community_area.area_numbe=community_area.area_numbe.astype('int64')
print("_"*50)
print("boundaries_community.area_numbe dtype:",community_area.area_numbe.dtypes)
pubicHealth_gpd=community_area.merge(pubicHealth_Statistic,left_on='area_numbe', right_on='Community Area')

from mpl_toolkits.axes_grid1 import make_axes_locatable
import matplotlib.pyplot as plt
fig, axs=plt.subplots(1,2,figsize=(40, 40))

#Print the first set of data
divider=make_axes_locatable(axs[0]) 
cax_0=divider.append_axes("left", size="5%", pad=0.1) #Configure legend parameters
pubicHealth_gpd.plot(column='Lung Cancer',ax=axs[0],cax=cax_0,legend=True,cmap='OrRd')  #If the following error is prompted, the corresponding library needs to be installed according to the information. ImportError: The descartes package is required for plotting polygons in geopandas. You can install it using 'conda install -c conda-forge descartes' or 'pip install descartes'.
axs[0].set_title("Lung Cancer-Per 100,000 males (age adjusted)")
pubicHealth_gpd.apply(lambda x: axs[0].annotate(s=x["Lung Cancer"], xy=x.geometry.centroid.coords[0], ha='center'),axis=1) #增加标注

#Print the second set of data
divider=make_axes_locatable(axs[1])
cax_1=divider.append_axes("right", size="5%", pad=0.1)
pubicHealth_gpd.plot(column='Per Capita Income',ax=axs[1],cax=cax_1,legend=True,cmap='OrRd')
axs[1].set_title("Per Capita Income-2011 inflation-adjusted dollars")
pubicHealth_gpd.apply(lambda x: axs[1].annotate(s=x["Community Area Name"], xy=x.geometry.centroid.coords[0], ha='center'),axis=1)

plt.show()
```

    area           float64
    area_num_1      object
    area_numbe      object
    comarea        float64
    comarea_id     float64
    community       object
    perimeter      float64
    shape_area     float64
    shape_len      float64
    geometry      geometry
    dtype: object
    __________________________________________________
    boundaries_community.area_numbe dtype: int64
    


<a href=""><img src="./imgs/6_7.png" height="auto" width="auto" title="caDesign"></a>


### 1.3.2 Correlation analysis of public health data
Looking at the graph above, it seems that lung cancer incidence correlates with income. Areas with high incomes have a lower incidence of lung cancer, whereas low-income areas have a higher incidence of lung cancer. A rough observation can only give a preliminary judgment result that is not very certain. If it is necessary to prove whether there is a correlation utilizing data analysis, a correlation analysis is needed. 


```python
pubicHealth_Statistic_mapping={'Community Area':'CommunityArea', 
                                'Community Area Name':'CommunityArea_Name',
                                'Birth Rate':'Birth_Rate',
                                'General Fertility Rate':'General_FertilityRate',
                                'Low Birth Weight':'Low_BirthWeight',
                                'Prenatal Care Beginning in First Trimester':'PrenatalCareBeginning_inFirstTrimester', 
                                'Preterm Births':'Preterm_Births',
                                'Teen Birth Rate':'TeenBirth_Rate',
                                'Assault (Homicide)':'Assault_Homicide',
                                'Breast cancer in females':'BreastCancer_infemales',
                                'Cancer (All Sites)':'Cancer_AllSites', 
                                'Colorectal Cancer':'Colorectal_Cancer',
                                'Diabetes-related':'Diabetes_related',
                                'Firearm-related':'Firearm_related',
                                'Infant Mortality Rate':'InfantMortality_Rate', 
                                'Lung Cancer':'Lung_Cancer',
                                'Prostate Cancer in Males':'ProstateCancer_inMales',
                                'Stroke (Cerebrovascular Disease)':'Stroke_CerebrovascularDisease',
                                'Childhood Blood Lead Level Screening':'ChildhoodBloodLeadLevel_Screening',
                                'Childhood Lead Poisoning':'ChildhoodLead_Poisoning',
                                'Gonorrhea in Females':'Gonorrhea_inFemales', 
                                'Gonorrhea in Males':'Gonorrhea_inMales', 
                                'Tuberculosis':'Tuberculosis',
                                'Below Poverty Level':'BelowPoverty_Level', 
                                'Crowded Housing':'Crowded_Housing', 
                                'Dependency':'Dependency',
                                'No High School Diploma':'NoHighSchool_Diploma', 
                                'Per Capita Income':'PerCapita_Income',
                                'Unemployment':'Unemployment',
                                }
pubicHealth_rename=pubicHealth_gpd.rename(columns=pubicHealth_Statistic_mapping)
pubicHealth_extract_columns=[
       'Birth_Rate','General_FertilityRate', 'Low_BirthWeight',
       'PrenatalCareBeginning_inFirstTrimester', 'Preterm_Births',
       'TeenBirth_Rate', 'Assault_Homicide', 'BreastCancer_infemales',
       'Cancer_AllSites', 'Colorectal_Cancer', 'Diabetes_related',
       'Firearm_related', 'InfantMortality_Rate', 'Lung_Cancer',
       'ProstateCancer_inMales', 'Stroke_CerebrovascularDisease',
       'ChildhoodBloodLeadLevel_Screening', 'ChildhoodLead_Poisoning',
       'Gonorrhea_inFemales', 'Gonorrhea_inMales', 'Tuberculosis',
    
       'BelowPoverty_Level', 'Crowded_Housing', 'Dependency',
       'NoHighSchool_Diploma', 'PerCapita_Income', 'Unemployment'    
]
pubicHealth_extract=pubicHealth_rename[pubicHealth_extract_columns]
publicHealth_correlation=pubicHealth_extract.corr()
print_html(publicHealth_correlation)
```




<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Birth_Rate</th>
      <th>General_FertilityRate</th>
      <th>Low_BirthWeight</th>
      <th>PrenatalCareBeginning_inFirstTrimester</th>
      <th>Preterm_Births</th>
      <th>TeenBirth_Rate</th>
      <th>Assault_Homicide</th>
      <th>BreastCancer_infemales</th>
      <th>Cancer_AllSites</th>
      <th>Colorectal_Cancer</th>
      <th>Diabetes_related</th>
      <th>Firearm_related</th>
      <th>InfantMortality_Rate</th>
      <th>Lung_Cancer</th>
      <th>ProstateCancer_inMales</th>
      <th>Stroke_CerebrovascularDisease</th>
      <th>ChildhoodBloodLeadLevel_Screening</th>
      <th>ChildhoodLead_Poisoning</th>
      <th>Gonorrhea_inFemales</th>
      <th>Tuberculosis</th>
      <th>BelowPoverty_Level</th>
      <th>Crowded_Housing</th>
      <th>Dependency</th>
      <th>NoHighSchool_Diploma</th>
      <th>PerCapita_Income</th>
      <th>Unemployment</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Birth_Rate</th>
      <td>1.000000</td>
      <td>0.810334</td>
      <td>0.108179</td>
      <td>-0.178847</td>
      <td>0.004334</td>
      <td>0.612710</td>
      <td>0.188536</td>
      <td>-0.022201</td>
      <td>-0.001289</td>
      <td>-0.097507</td>
      <td>0.303831</td>
      <td>0.162722</td>
      <td>0.051716</td>
      <td>-0.063567</td>
      <td>0.130114</td>
      <td>0.126423</td>
      <td>0.481725</td>
      <td>0.210016</td>
      <td>0.126671</td>
      <td>0.279755</td>
      <td>0.249764</td>
      <td>0.603219</td>
      <td>0.115771</td>
      <td>0.594249</td>
      <td>-0.377653</td>
      <td>0.149439</td>
    </tr>
    <tr>
      <th>General_FertilityRate</th>
      <td>0.810334</td>
      <td>1.000000</td>
      <td>0.142189</td>
      <td>-0.134292</td>
      <td>0.122235</td>
      <td>0.655528</td>
      <td>0.293843</td>
      <td>0.058089</td>
      <td>0.164279</td>
      <td>0.022977</td>
      <td>0.352294</td>
      <td>0.285562</td>
      <td>0.165320</td>
      <td>0.026357</td>
      <td>0.196524</td>
      <td>0.241311</td>
      <td>0.490693</td>
      <td>0.303624</td>
      <td>0.250984</td>
      <td>0.144972</td>
      <td>0.173846</td>
      <td>0.655826</td>
      <td>0.524558</td>
      <td>0.714008</td>
      <td>-0.661131</td>
      <td>0.275589</td>
    </tr>
    <tr>
      <th>Low_BirthWeight</th>
      <td>0.108179</td>
      <td>0.142189</td>
      <td>1.000000</td>
      <td>-0.532546</td>
      <td>0.843100</td>
      <td>0.622936</td>
      <td>0.747493</td>
      <td>0.425041</td>
      <td>0.759791</td>
      <td>0.657992</td>
      <td>0.664107</td>
      <td>0.671880</td>
      <td>0.677792</td>
      <td>0.671295</td>
      <td>0.713754</td>
      <td>0.525439</td>
      <td>0.416265</td>
      <td>0.388319</td>
      <td>0.737727</td>
      <td>0.125231</td>
      <td>0.681049</td>
      <td>-0.080532</td>
      <td>0.479581</td>
      <td>0.056962</td>
      <td>-0.394564</td>
      <td>0.702227</td>
    </tr>
    <tr>
      <th>PrenatalCareBeginning_inFirstTrimester</th>
      <td>-0.178847</td>
      <td>-0.134292</td>
      <td>-0.532546</td>
      <td>1.000000</td>
      <td>-0.413383</td>
      <td>-0.579170</td>
      <td>-0.591605</td>
      <td>-0.142970</td>
      <td>-0.385923</td>
      <td>-0.315996</td>
      <td>-0.470152</td>
      <td>-0.540972</td>
      <td>-0.466319</td>
      <td>-0.353354</td>
      <td>-0.430259</td>
      <td>-0.395817</td>
      <td>-0.328470</td>
      <td>-0.516579</td>
      <td>-0.653985</td>
      <td>-0.270147</td>
      <td>-0.516145</td>
      <td>-0.074951</td>
      <td>-0.192405</td>
      <td>-0.078027</td>
      <td>0.200858</td>
      <td>-0.542603</td>
    </tr>
    <tr>
      <th>Preterm_Births</th>
      <td>0.004334</td>
      <td>0.122235</td>
      <td>0.843100</td>
      <td>-0.413383</td>
      <td>1.000000</td>
      <td>0.549843</td>
      <td>0.742019</td>
      <td>0.435020</td>
      <td>0.780207</td>
      <td>0.722153</td>
      <td>0.650790</td>
      <td>0.732584</td>
      <td>0.672087</td>
      <td>0.675119</td>
      <td>0.642100</td>
      <td>0.539608</td>
      <td>0.289144</td>
      <td>0.379146</td>
      <td>0.800491</td>
      <td>0.001197</td>
      <td>0.550906</td>
      <td>-0.140244</td>
      <td>0.571350</td>
      <td>-0.023655</td>
      <td>-0.364807</td>
      <td>0.646550</td>
    </tr>
  </tbody>
</table>



The above correlation calculation has calculated the correlation coefficients between the two pairs. And to see the correlation coefficients more clearly, a scatter plot can be printed at the same time (usually before the correlation coefficient is calculated to see the linear relationship between two variables), as well as a thermal diagram that makes it easier to see the variation of the correlation coefficient.


```python
import plotly.express as px
fig=px.scatter_matrix(pubicHealth_extract)

fig.update_layout(
    autosize=True,
    width=1800,
    height=1800,
    )
fig.show()
```

<a href=""><img src="./imgs/6_8.png" height="auto" width="auto" title="caDesign"></a>

```python
import seaborn as sns
plt.figure(figsize=(20, 20))
sns.heatmap(publicHealth_correlation,annot=True, fmt=".2f", linewidths=.5,)
plt.show()
```


<a href=""><img src="./imgs/6_9.png" height="auto" width="auto" title="caDesign"></a>


Whether it is a scatter diagram or a heat diagram or a correlation coefficient matrix, it is not easy to observe the object concerned when observing multiple pairs' correlation. Therefore, the variables that need to be paid attention to can be extracted according to the purpose of analysis. Only the correlation coefficient between this variable and all other variables can be concerned.


```python
import numpy as np
economic_factors=['BelowPoverty_Level', 'Crowded_Housing', 'Dependency', 'NoHighSchool_Diploma', 'PerCapita_Income', 'Unemployment']
publicHealth_indicator=publicHealth_correlation[economic_factors]
publicHealth_indicator_columns=publicHealth_indicator.T.columns.to_numpy()

plt.rcdefaults()
plt.rcParams.update({'font.size':9})

nrows=3
ncols=2
fig, axs=plt.subplots(nrows=nrows,ncols=ncols,figsize=(10*2, 10*2))  
y_pos=np.arange(len(publicHealth_indicator_columns))

i=0
for idx in [(row,col) for row in range(nrows) for col in range(ncols)]:
    axs[idx].barh(y_pos, publicHealth_indicator[economic_factors[i]].to_numpy(), align='center') 
    axs[idx].set_yticks(y_pos)
    axs[idx].set_yticklabels(publicHealth_indicator_columns)
    axs[idx].invert_yaxis()  # labels read top-to-bottom
    axs[idx].set_xlabel(economic_factors[i])
    #axs.set_title(title_str)
    for index, value in enumerate(publicHealth_indicator[economic_factors[i]].to_numpy()):
        if value>=0:
            axs[idx].text(value, index, str(round(value,2)),horizontalalignment='left')
        else:
            axs[idx].text(value, index, str(round(value,2)),horizontalalignment='right')
    i+=1
    
plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.1,wspace=0.4)
plt.show()
```


<a href=""><img src="./imgs/6_10.png" height="auto" width="auto" title="caDesign"></a>


Although the correlation coefficient between two public indicators is calculated, it does not mean the correlation calculated by the sample can reflect whether the phenomenon obtained by the sample also exists in the population. It is necessary to pay attention to the p-value calculated by the hypothesis test. Eliminating the two variables does not meet the significance level of 0.05 to obtain the part that can reflect the overall correlation. First, the variables We care about are used as the index, for all economic variables, `economic_factors=['BelowPoverty_Level', 'Crowded_Housing', 'Dependency', 'NoHighSchool_Diploma', 'PerCapita_Income', 'Unemployment']`, to facilitate subsequent analysis. 


```python
from scipy import stats
disease_columns=[       
       'Birth_Rate','General_FertilityRate', 'Low_BirthWeight',
       'PrenatalCareBeginning_inFirstTrimester', 'Preterm_Births',
       'TeenBirth_Rate', 'Assault_Homicide', 'BreastCancer_infemales',
       'Cancer_AllSites', 'Colorectal_Cancer', 'Diabetes_related',
       'Firearm_related', 'InfantMortality_Rate', 'Lung_Cancer',
       'ProstateCancer_inMales', 'Stroke_CerebrovascularDisease',
       'ChildhoodBloodLeadLevel_Screening', 'ChildhoodLead_Poisoning',
       'Gonorrhea_inFemales', 'Gonorrhea_inMales', 'Tuberculosis',
        ]
pubicHealth_pearsonr={}

for factor in economic_factors:
    disease_temp={}
    for disease in disease_columns:
        desease_series=pd.to_numeric(pubicHealth_extract[disease],errors='coerce')
        factor_series=pd.to_numeric(pubicHealth_extract[factor],errors='coerce')
        mask=pd.notna(desease_series)&pd.notna(factor_series)
        
        disease_temp[disease]=stats.pearsonr(pd.to_numeric(factor_series[mask],errors='ignore'),pd.to_numeric(desease_series[mask],errors='ignore'))
    pubicHealth_pearsonr[factor]=disease_temp
pubicHealth_pearsonr_df=pd.DataFrame.from_dict(pubicHealth_pearsonr, orient='index').stack().to_frame(name='corr_pV')

pubicHealth_pearsonr_df['correlation']=pubicHealth_pearsonr_df.corr_pV.apply(lambda row:row[0])
pubicHealth_pearsonr_df['p_value']=pubicHealth_pearsonr_df.corr_pV.apply(lambda row:row[1])

print_html(pubicHealth_pearsonr_df)
```




<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>corr_pV</th>
      <th>correlation</th>
      <th>p_value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="5" valign="top">BelowPoverty_Level</th>
      <th>Birth_Rate</th>
      <td>(0.24976406537114698, 0.028475743860025216)</td>
      <td>0.249764</td>
      <td>2.847574e-02</td>
    </tr>
    <tr>
      <th>General_FertilityRate</th>
      <td>(0.17384625982353086, 0.13051281178487212)</td>
      <td>0.173846</td>
      <td>1.305128e-01</td>
    </tr>
    <tr>
      <th>Low_BirthWeight</th>
      <td>(0.6810485083790097, 9.384671983490635e-12)</td>
      <td>0.681049</td>
      <td>9.384672e-12</td>
    </tr>
    <tr>
      <th>PrenatalCareBeginning_inFirstTrimester</th>
      <td>(-0.5161448655124204, 1.5503295893976543e-06)</td>
      <td>-0.516145</td>
      <td>1.550330e-06</td>
    </tr>
    <tr>
      <th>Preterm_Births</th>
      <td>(0.5509056684724956, 2.086303329322895e-07)</td>
      <td>0.550906</td>
      <td>2.086303e-07</td>
    </tr>
  </tbody>
</table>



With the value of the correlation coefficient as the x-coordinate, the corresponding p-value of the correlation coefficient as the y-coordinate, and 0.05 as the significance level, the scatter plot can be printed to visually check which pairwise correlation coefficient of public health indicators can reflect or cannot reflect the widespread phenomenon.


```python
pubicHealth_pearsonr_resetIdx=pubicHealth_pearsonr_df.reset_index()
significance_level=0.05
fig=plt.figure(figsize=(30, 10))
ax=fig.add_subplot(111, facecolor='#FFFFCC')
X=pubicHealth_pearsonr_resetIdx.correlation
Y=pubicHealth_pearsonr_resetIdx.p_value
ax.plot(X, Y, 'o')
ax.axhline(y=significance_level,color='r',linestyle='--',label='significance_level')
i=0
for x,y in zip (X,Y):
    label=pubicHealth_pearsonr_resetIdx.loc[i].level_0+' : '+pubicHealth_pearsonr_resetIdx.loc[i].level_1
    plt.annotate(label, # this is the text
                 (x,y), # this is the point to label
                 textcoords="offset points", # how to position the text
                 xytext=(0,10), # distance from text to points (x,y)
                 ha='center', # horizontal alignment can be left, right or center
                 rotation=90
                )
    
    i+=1
plt.annotate('alpha level=%.2f'%significance_level,xy=(-0.7,significance_level),xytext=(-0.75, 0.15),arrowprops=dict(facecolor='black', shrink=0.01),horizontalalignment='right',)
plt.xlabel("correlation")
plt.ylabel("p_value")    
plt.show()
```


<a href=""><img src="./imgs/6_11.png" height="auto" width="auto" title="caDesign"></a>


All row with a p-value less than or equal to 0.05 were extracted, and the correlation coefficient results were printed in the form of a histogram.


```python
#All row with a p-value less than or equal to 0.05 were extracted
pubicHealth_pearsonr_alpha=pubicHealth_pearsonr_df[pubicHealth_pearsonr_df.p_value<=0.05]
print_html(pubicHealth_pearsonr_alpha)
```




<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>corr_pV</th>
      <th>correlation</th>
      <th>p_value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="5" valign="top">BelowPoverty_Level</th>
      <th>Birth_Rate</th>
      <td>(0.24976406537114698, 0.028475743860025216)</td>
      <td>0.249764</td>
      <td>2.847574e-02</td>
    </tr>
    <tr>
      <th>Low_BirthWeight</th>
      <td>(0.6810485083790097, 9.384671983490635e-12)</td>
      <td>0.681049</td>
      <td>9.384672e-12</td>
    </tr>
    <tr>
      <th>PrenatalCareBeginning_inFirstTrimester</th>
      <td>(-0.5161448655124204, 1.5503295893976543e-06)</td>
      <td>-0.516145</td>
      <td>1.550330e-06</td>
    </tr>
    <tr>
      <th>Preterm_Births</th>
      <td>(0.5509056684724956, 2.086303329322895e-07)</td>
      <td>0.550906</td>
      <td>2.086303e-07</td>
    </tr>
    <tr>
      <th>TeenBirth_Rate</th>
      <td>(0.6600382763346118, 6.598083934767898e-11)</td>
      <td>0.660038</td>
      <td>6.598084e-11</td>
    </tr>
  </tbody>
</table>




```python
import plotly.graph_objects as go
import random

def generate_colors():
    import matplotlib
    '''
    function - Generate a color list or dictionary
    '''
    hex_colors_dic={}
    rgb_colors_dic={}
    hex_colors_only=[]
    for name, hex in matplotlib.colors.cnames.items():
        hex_colors_only.append(hex)
        hex_colors_dic[name]=hex
        rgb_colors_dic[name]=matplotlib.colors.to_rgb(hex)
    return hex_colors_only,hex_colors_dic,rgb_colors_dic
generate_colors,_,_=generate_colors()

economic_factors=['BelowPoverty_Level', 'Crowded_Housing', 'Dependency', 'NoHighSchool_Diploma', 'PerCapita_Income', 'Unemployment']
fig = go.Figure()

for idx,data in pubicHealth_pearsonr_alpha.groupby(level=0):
    fig.add_trace(go.Bar(x=disease_columns,
                    y=data.correlation,
                    name=idx,
                    marker_color=random.choice(hex_colors_only)
                    ))

fig.update_layout(
    title='Public health indicators_correlation,p-value<0.05',
    xaxis_tickfont_size=14,
    yaxis=dict(
        title='correlation)',
        titlefont_size=16,
        tickfont_size=14,
    ),
    barmode='group',
    bargap=0.1, # gap between bars of adjacent location coordinates.
    bargroupgap=0.5 # gap between bars of the same location coordinate.
)
fig.show()
```

<a href=""><img src="./imgs/6_12.png" height="auto" width="auto" title="caDesign"></a>


### 1.4 key point
#### 1.4.1 data processing technique

* The statistics part of the calculation is mainly based on the Scipy(.stats) library and the Numpy library.

* Add annotations to the GeoDataFrame format data chart print. pubicHealth_gpd.apply(lambda x: axs[0].annotate(s=x["Lung Cancer"], xy=x.geometry.centroid.coords[0], ha='center'),axis=1) 

#### 1.4.2 The newly created function tool

* function -  Print DataFrame format data in Jupyter as HTML, `print_html(df,row_numbers=5`

* function - Generate a color list or dictionary,  `generate_colors()`

#### 1.4.3 The python libraries that are being imported


```python
import numpy as np
from scipy import stats
import seaborn as sns
import math
import matplotlib.pyplot as plt 
import pandas as pd
from scipy.stats import norm
from scipy.stats import t
from IPython.display import HTML
import geopandas as gpd
import plotly.express as px
```

#### 1.4.4 Reference
1. Timothy C.Urdan.Statistics in Plain English. Routledge; 3rd Edition (May 27, 2010)
2. Shin Takahashi.The Manga Guide to Statistics.  No Starch Press; 1st Edition (November 15, 2008)
